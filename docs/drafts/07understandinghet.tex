\section{Understanding the roots of heterogeneity} \label{sec:roots_of_heterogeneity}
In the previous section, we discussed the conditional average treatment effect of each individual given their specific set of characteristics. Prior contributions have looked at the correlations between the significance of the estimated MPC and households characteristics to get a glimpse into which factors play a role in the MPC. However, this approach does not reliably tell us which variables really drive the response. The correlation might very well be spurious or driven by other factors that are correlated with the characteristic we are looking at. Therefore, it is more fruitful to look at measures that can help us identify what role a variable plays in our predicted MPCs. In the case of specifications using the linear DML estimator, we know that this relationship is linear by construction since the CATE is defined as a linear combination of the single effects of interactions between treatment and the respective variable (see equation (\ref{eq:CATE})). However, the causal forest-based approach will help us reveal whether there are any non-linear patterns underlying the effect of characteristics on the MPC without assuming any functional form of these patterns. \\
For this, we turn to the Machine Learning literature, which has developed a number of tools to analyze the relationship between prediction and feature. Feature is a different term for control variables used in the Machine Learning literature. In our setting, these are the variables we condition on to find the CATE, i.e., variables contained in $X$. Since variables in $W$ are assumed to not impact the CATE, they are not contained in the second stage and therefore play no role in predicting individuals' MPC. Machine Learning estimators such as random forests are black boxes as they only provide predictions but stay quiet on which variables are important to arrive at this prediction. The literature has proposed multiple approaches that help quantify the role of a single feature, some of which we look at in the following. 

\subsection{Marginal and Partial Dependence Plots}
\textbf{this section is quite long for something I do not show, right?} \\
Two popular approaches are marginal plots (M-Plots) and Partial Dependence Plots (PDPs; Friedman, 2001). Both use the same general idea to quantify the impact of some feature $x_S$ on our predictions: we replace the value of $x_S$ of each observation with some value $v_1$. Then we fit our trained prediction model to this "counterfactual" dataset and take the average over all these predictions. For example, we predict for each individual what their MPC looks like if they had a certain age and average the predicted MPC. Then we continue with $x_S=v_2$ and so on, where the values $v_j$ are chosen from a grid along the distribution of $x_S$. The difference between M-Plots and PDPs is the distribution of all other features $X_C=X\setminus x_S$ we average over. In the case of Marginal Plots, contrary to what the name might suggest, we use the conditional distribution of $X_C$ given $x_S$, $p_{X_C|x_S}$, to obtain the impact of $x_S$ on our prediction. On the other hand, PDPs use the marginal distribution of $X_C$, $p_{X_C}$. 
% \begin{align}
% \hat{f}_M(x_S=v_j)=\int p_{X_C|x_S=v_j} m(x_S=v_j, X_C)dX_C, 
% \end{align}
% where $m$ is our predictor and $\hat{m}_M(v_j)$ is the effect at $x_S=v_j$. 
% \begin{align}
%     \hat{f}_{PDP}(x_S)=\int p_{X_C} f(x_S, X_C)dX_C \label{eq:pdp}
% \end{align}
% where $\hat{m}_{PDP}(v_j)$ is the Partial Dependence of our predictor on $x_S$ at $v_j$. 
Partial Dependence Plots are more common in the Machine Learning literature as M-Plots suffer from a severe weakness when features in $X_C$ are correlated with $x_S$. However, PDPs also fail to reliably uncover the effect of $x_S$ in such a setting. \\
To illustrate the issues arising in M-Plots and PDPs when features are correlated, let us consider a simple example. Let's say we have some predictive model $m$ that only depends on two predictors $x_1$ and $x_2$, which are positively correlated. To now calculate the M-Plot of $x_1$ at $v_1$ we use the conditional distribution $p_{x_2|x_1}$. In practice, we plug in $x_1=v_1$ for each observation that is within a specified neighborhood of $x_1=v_1$ (e.g. observations in the same quantile). Then we predict and average to obtain the M-Plot value at $x_1=v_1$. Repeating this procedure for other values $v_j$ then results in the M-Plot of $x_1$. However, because the two variables are correlated, we do not know which variable drives the observed effect - if $x_1$  is increased, the values of $x_2$  we use for our predictions also increase because we only use $x_2$ of observations that are close to having $x_1=v_j$. This problem is known as 'conflation.' \\
On the other hand, Partial Dependence Plots do not suffer from this problem because they use the marginal distribution of $x_2$. We use all observations of $x_2$ instead of only looking at a neighborhood in which $x_1=v_1$ and, therefore, average out the effect of $x_2$ on our predictions. Since we use the same set of $x_2$ values at each point $v_j$, we know that changes in our predictions must stem from $x_1.$ Still, the PDPs are not a good tool when features are correlated, and this is connected to the machine learning estimators we apply them to. These are nonparametric estimators that are usually very weak in predicting outcomes based on observations they have never seen before. This extrapolation, however, becomes necessary when we create the "counterfactual" dataset by setting $x_1=v_j$. By doing so, we effectively create observations that are extremely unlikely or even impossible to observe in the real world because of the correlation between the features. For example, in our data, age and salary are strongly correlated, which is quite intuitive because once retired, households do not receive a salary anymore. When creating PDPs, we ignore this fact and create households that have a high salary and are very old. The weakness in extrapolation leads the model to create imprecise predictions, which then severly bias the Partial Dependence Plots. (Apley and Zhu, 2020)  \\
Therefore, while PDPs do not suffer from theoretical drawbacks like M-Plots, in practice, they are unable to uncover the effects of $x_1$ on our predictions in a stable manner because of the underlying predictive estimator. If the true model is indeed linear and we use a linear prediction method with the correct specifications of any interaction terms etc., then this extrapolation issue is unlikely to occur. Moreover, by construction, a linear predictor will result in linear Partial Dependence Plots. 

\subsection{Accumulated Local Effects}
To circumvent issues arising in M-Plots and PDPs from correlated features, Apley and Zhu (2019) propose Accumulated Local Effects (ALE). The extrapolation issue PDPs suffer from is bypassed by using the conditional distribution $p_{X_C|x_S}$ as we do in M-Plots. The 'conflation' effect that results from this is tackled by not using average predictions at $x_S=v_j$ but rather the average marginal change in predictions at this point. In other words, we use the partial derivative of our predictor in question $m$ with respect to $x_S$ at the point $v_j$. Although many machine learning methods such as tree-based learners have no concept of a gradient, Apley and Zhu are able to derive proofs for non-differentiable functions $m$ (see Section X.X in Apley and Zhu), and further does this not play a role when it comes to the ALE estimation. The ALE is then defined as
\begin{align}
\hat{m}_{S, ALE} (x_S)=\int_{z_{0, S}}^{x_S} E_{X_C|X_S=x_S}[\hat{m}^S(X_S, X_C)|X_S=z_S]dz_S - constant, \label{eq:ale}.
\end{align}
Looking at this equation step-by-step reveals how the ALE recovers the effect of $x_S$ on our predictions even when features are correlated. As already mentioned, the ALE avoids 'conflation' by using the partial derivative of $m$, where we have $m^S=\frac{\partial m}{\partial x_S}\rvert_{x_S=v_j}$ as the partial derivative of $m$ evaluated at the point we want to find the ALE for. Since we only look at an infinitesimally small change, this change in $x_S$ will not affect the features that are correlated with it in $X_C$ unless the correlation is extremely high. In our analysis, we would want to avoid this case anyways to avoid problems in the estimation itself (e.g., multicollinearity). Once the changes in prediction are obtained for each observation, we average them over the conditional distribution, i.e., only using observations that are within a neighborhood of $x_S=v_j$ and actually exist. Now we have the average local effect of $x_S$ on our prediction. To better visualize the global role of this feature, Apley and Zhu argue that this can be achieved by accumulating all local effects up to $x_S=v_j$. Thus, we simply integrate overall local effects up to $x_S=v_j$, where $z_{0, S}$ is the lower bound of the distribution of $x_S$.\footnote{For more on this, see Section 5.2 of Apley and Zhu (2019) where they demonstrate how accumulation helps to improve the interpretability of ALE plots.} \\
To estimate the ALE we use the following estimator, which illustrates the procedure in more intuitive terms: 
\begin{align}
\hat{\tilde{m}}_{j, ALE}(x_S=v_j)=\sum_{k=1}^{k_j(x_S=v_j)} \frac{1}{n_j(k)}\sum_{i:x_{i,j}\in N_j(k)}[m(z_{k,j}, x_{i,\setminus j})-m(z_{k-1,j}, x_{i,\setminus j})] \label{eq:ale_estim}
\end{align}
The intuition behind the estimator is straightforward. First, we bin our data into $n_b$ bins based on quantiles of the distribution of $x_S$. To mimic the marginal change represented by the partial derivative $m^S$ in \ref{eq:ale} we make two predictions for each individual. For an observation $i$ that falls in bin $k$, we predict its outcome with $x_S=z_k$ and $x_S=z_{k-1}$, where $z_k$ represents the upper bound value of bin $k$. We then average overall individuals that fall within this bin $k$ denoted in equation (11) by the neighborhood $N_j(k)$. Finally, we accumulate all predicted preferences over all bins up to the bin at which point x falls, denoted by k(x). Only looking at individuals within the neighborhood $N(k)$ accounts for the conditional distribution used in \ref{eq:ale}. \\
As the last step, Apley and Zhu propose to center the effect around the average of all ALEs such that the mean effect is zero. Thus, the ALE has to be interpreted relative to the average prediction, and it shows whether for a given $x_S=v$ the effect of $x_S$ is above or below the average prediction. I.e., whether $x_S$ affects our predictions at $x_S=v$ more than it does on average. In practice, the $constant$ in (\ref{eq:ale}) is replaced by 
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n \hat{m}_{j, ALE}(x_{i, j}).
\end{align*}
Summarizing, the ALE shows the effect of a 
Note that we yet cannot say something meaningful about the statistical significance of these results. Most fields are only interested in the predictive power of machine learning methods and in understanding how these predictions are achieved, but there is no notion of statistical significance in these settings. Therefore, a specific approach to quantify the uncertainty of these measures has not yet been developed. However, a deeper look into this topic is out of the scope of this work. To briefly dive into the topic of statistical significance, we use a bootstrapping-based approach. We simulate the ALEs for $n_{bootstrap}$ samples. These create an empirical distribution on which basis we can calculate a pseudo Confidence interval using the reverse percentile approach (Davison and Hinkley, 1997, p. 194 eq 5.6). We report these as the red lines surrounding the ALE in Figures X.X. We see that these bands are very wide in certain parts - especially in areas where there is a small number of observations. We strongly encourage a deeper investigation of the statistical properties of ALEs and a potential way of quantifying their uncertainty in a more rigorous way. While the ALEs show us the relationship between a specific variable and our predictions, having a sound theoretical foundation on which basis we could assess the statistical significance of these relationships is desirable.t. 

\subsection{Results}
We now turn to the analysis of the Accumulated Local Effects of a selection of features used in our estimated specifications. As pointed out in Section 6.1, by the construction of the estimator and how the ALE is calculated, it will always depict a linear relationship when looking at the linear DML setting. However, we can still infer in what direction the relationship is going - e.g., whether predictions are above or below average for young people. More importantly, it is useful as a benchmark to compare the ALE of our estimates using the causal forest as the second stage estimator. \\
Two main channels of MPC heterogeneity discussed in the literature are the role of age and liquidity. We start by investigating the role of age and plot the ALE for households' age with respect when using changes in non-durable consumption as the outcome in Figure X.X.
Two things become evident right away: First, the linear model finds that young people have a substantially lower MPC, while older households experience stronger reactions. Second, the relationship more or less breaks down once we control for liquidity, salary, and income. This is observed across all three main consumption categories. While the relationship actually turns around for strictly non-durables and total consumption, the non-durables still seem to associate a positive relationship between age and MPC. However, we see that the deviations from the average predictions decline drastically and are almost zero. Taking a look at the difference between SND and ND categories, we see that in the case of health expenditures, the MPC has a positive relationship with higher age that actually strengthens once we include liquidity, income, and salary (see Figure X.X in Appendix A.A). The effect seems to be strong enough to keep a linearly increasing relationship between MPC and age in the ND consumption category, while this does not appear in the SND category. In the case of total expenditure, the effect does not seem to be strong enough compared to the overall direction of the relationship between MPC predictions and age. \\
Still, we have established in Section 5.3 that the causal forest estimator reveals more significant MPCs in specification 3, where we control for liquidity, which is likely to occur because of nonlinearities not picked up in the linear CATE model. 
Instead of a clear linear relationship, the ALE plots for AGE when using the causal forest estimator reveal a quite more varying relationship. Similar to the linear model, the overall structure of the relationship in specifications 1 and 2 is similar across all three main consumption categories. However, the magnitude of the ALEs varies a lot. In the case of total expenditures, we see large effects, while they decline more and more once we reduce the number of goods categories considered. Additionally, it is clearly visible that the ALEs for the causal forest model are, in part, varying widely. Mostly, the 95 and 5 percentiles of our bootstrapped ALEs are so wide that we cannot boldly state that the effects are positive or negative as our pseudo-CIs include zero along most parts of the age distribution. While this is not a valid statement on any hypothesis testing, it hints at a rather unstable relationship between age and the MPC. Once we turn to specification 3, the CIs become much more narrow but still include zero. Moreover, the widely varying ALEs are more closely fluctuating around zero. The only range where we find CI bounds above zero is in the case of TOT expenditures. This effect vanishes once we look at the less aggregated measures. Thus, we take a closer look into the sub-categories that are only included in TOT. We have to stress that these are not causal relationships we establish between any of these ALEs, but we only try to infer directions from which the effects we find in aggregated measures might stem. \\
Summarizing the ALEs of age, it is reasonable to say that the linear model fails to account for the correct relationship between age and MPC, while our findings support that households at the upper end of the age distribution experience higher MPCs out of the rebate shock than on average. As we have laid out in Section 6.1, we cannot make any substantial claims on whether these reactions are significantly different from zero but rather only look at the role age plays for our MPC predictions. In our case, this means that a higher age implies that a household's MPC will be larger than the average MPC of other households. \\
The main channel identified in the literature so far is liquidity. Our discussion of the underlying theory of binding borrowing constraints and lacking access to liquid assets provides the intuition for the following analysis. We consider the change in non-durable consumption here but note that this pattern is evident across all main consumption categories. ALE plots for these can be found in Appendix A.A. Figure X.X provides the ALE plots for specification 3, which includes liquidity, income, and salary for both estimation procedures. \\
Using the linear estimator, we find that the predictions rise in liquidity; however, the deviations calculated are very small - remember though we cannot test whether they are significant. The causal forest estimations are more informative. Here we see a strong spike in low levels of liquidity that falls as rapidly as it rises once liquidity is sufficiently large (around 5000). This underlines the important role of liquidity documented in the literature and by our results presented in Section 5.2, where we have seen that controlling for and conditioning on liquidity has a large impact on the MPCs and their significance. The ALE now provides further hints on what the role of liquidity might look like. We see that for very low levels of liquidity, the reaction of households is even below average, while once it is slightly above zero, we see that there is a steep jump in the ALE, which more or less declines immediately again at increasing levels of liquidity. This potentially signals that households with no liquid assets at all will actually not react more strongly than the average household. Since we find that the average household does not significantly react, we infer from this that households with no liquidity are not reacting to the income shock. In our data, income and salary are positively correlated, i.e., households with low liquidity also have low income. Thus, it is reasonable to assume that for these households, the rebate checks make up a larger share of their total income within this quarter. Given the times of economic hardship for many of these households during the time of the survey, it is possible that these households did not spend a significant amount all within one quarter but stretched out their expenditures over several months, using the rebate checks as fall back savings. However, this is only one interpretation of our results, and our data is too limited to infer robust causal relationships and the existence of such channels. \\
The reaction for low liquidity households is in line with our expectations from the liquidity channel we discussed. As soon as households have enough liquid assets, it seems that they are capable of borrowing to smooth out the income shock before it arrives and thus shows no significant reaction in consumption. \\
Last, we would like to discuss the effect of income and salary on the predicted MPC. As we can see from the ALEs in Figure X.X (they display the causal forest estimates for spec 3 of chNDexp for salary and income), we observe that salary and income have a strongly negative impact on the predicted MPCs. 

\subsection{Takeaways from a policy-perspective}
From a policy perspective, it might not be of interest on which exact sub-category of consumption people spend their rebate on - at least when being interested in providing a stimulus to the economy overall. Still, our analysis reveals useful information for making stimuli more targeted to be more effective or to get a general sense of what people spend additional income on, given their characteristics. Although natural experiments such as the 2008 tax stimulus and connected analysis have mostly little external validity outside of their context, the heterogeneity analysis provides a hint on spending patterns and reactions of households given their personal characteristics and financial circumstances. This can at least be a starting point when designing more targeted transfer programs.