\section{Understanding the roots of heterogeneity} \label{subsec:roots_of_heterogeneity}
In the previous section we discussed the conditional average treatment effect of each individual given their specific set of characteristics. Similarly to prior contributions, we also looked at correlations between the significance of the estimated MPC and households characteristics to get a glimpse into which factors play a role in the MPC. However, this approach does not reliably tell us which variables really drive the response. The correlation might very well be spurious or driven by other factors that are correlated with the characteristic we are looking at. Therefore, it is more fruitful to look at measures that can help us identify what role a variable plays in our predicted MPCs. \\
For this, we turn to the Machine Learning literature, which has developed a number of tools to analyze the relationship between prediction and feature. A feature is the term used for control variables that are used to predict the outcome in the Machine Learning literature. In the following, we will use this term to describe our set of variables $X$ as those are the ones we condition on when prediction the MPC. Machine Learning estimators such as random forests are blackboxes as they only provide predictions but stay quiet on which variables are important to arrive at this prediction. The literature has proposed multiple approaches that help quantify the role of a single or multiple features, some of which we will look at in the following. \\

\subsection{Partial Dependence Plots - and why we don't look at them}
First, we implement Partial Dependence Plots (PDPs) developed by Friedman (2001). The idea of the PDP is rather simple: to understand the role of feature $x_S$ in our predictions, we replace the value of $x_S$ with some value $v_1$ and use our trained model to find a new set of predictions. For example, we predict for each individual what their MPC looks like if they had a certain age. Once the predictions are obtained, we take the average over all predictions and continue with the next value $v_2$ and so on. This creates a plot that shows how the values of $x_S$ affect our prediction. More formally, the PDP is defined as
\begin{align}
    \hat{f}_{x_S}=E[\hat{f}(x_S, X_C)]&=\int\hat{f}(x_S, X_C)dP(X_C) \label{eq:pdp} \\
    &=\int p_{X_C} \hat{f}(x_S, X_C)dX_C
\end{align}
where $\hat{f}$ is our predictor, $X_C$ the set of all features except $x_S$ and $p_{X_C}$ the marginal probability distribution of $X_C$. Note that we average over the marginal distribution of this set of variables. Therefore, the PDP estimates the average marginal effect of feature $x_S$ on our prediction $\hat{f}$. Again considering our example using age, a linearly increasing PDP would suggest that older individuals have a higher MPC - or, equivalently, that the MPC is increasing in age. \\
Averaging over the marginal distribution of $X_C$ implies that the distribution of $X_C$ at $x_C=v$  is the same as the marginal distribution of $X_C$ for any value $v$. Only as long as this assumption holds does the PDP reveal the true average marginal effect of the feature on our predictions. However, it immediately breaks down when our features are correlated. To illustrate this, let us consider the following example: the only variables that we condition on are age and liquidity and these are positively correlated - i.e. older households are more liquid. Then the PDP of age assumes that the age in our sample is equivalently distributed for high and low liquid households - an assumption that is obviously wrong. Now taking a look at (\ref{eq:pdp}) again, the issue becomes visible in theoretical terms as well. The integral represents a weighted average where we weight each prediction with the marginal probability of $X_C=x_C$ over the whole space of $X_C$. In terms of our example, the PDP for age=20 assigns the same weight to high liquid households as to low liquid households. However, in our data young households are not liquid and the probability assigned to predictions for young, liquid households should be lower than it actually is. \\
\textbf{Put the following into better words:}
So lets say young people have a lower MPC and more liquid people have a higher MPC than others (find an example where correlation is positive/negative and effect on MPC expected to be reversed/the same). Predictions for young, highly liquid people get a higher weight than they should and predictions of young, low liquid people get lower weights than they should. So, since liquidity pushes MPC up given some age, these higher MPC predictions will get a too large weight and actually the effect of liquidity is also captured through this. \\
Consider two features $x_1$ and $x_2$ that are correlated. If we now want to create the Partial Dependence Plot of $x_1$ at a point $v$, we replace $x_1$ with $v$ for each individual and implicitly assume that the distribution of $x_2$ is the same at every possible value of $v$ since we use the marginal distribution (rewording needed). However, since our features are correlated, this is not the case and we assign the wrong probabilities to these households (**here rewording and explain better what role of marginal distribution is/how this goes wrong in estimator)**. Instead, the distribution of $x_2$ might look very different for $x_1=v_1$ and $x_1=v_2$. This results in a potentially very severe bias. More intuitively, let us again consider our example with age and say that age is positively correlated with liquidity - older people are more liquid. When now looking at the PDP of age, we create high liquid, young households - households that are not observed in the real data because of the correlation. Therefore, the model does not know how to handle these households and the results are biased. \\
Indeed, results of the partial dependence plots are very spurious. They are reported in more detail in Appendix X.X, where we look at the PDPs for non-durable consumption. The effects have a high variation and point estimates out of line of the existing literature. 

\subsection{Accumulated Local Effects}
To circumvent issues arising in PDPs from correlated features, Apley and Zhu (XXX) propose Accumulated Local Effects (ALE). While the goal is the same, the approach to achieve differs substantially. ALEs average over the conditional distribution by considering bins of data, thus accounting for differences in the distribution of $x_2$ along $x_1$. We use Figure X.X to illustrate how we obtain ALEs. The figure shows $x_1$ on the x-axis and our prediction on the y-axis. First, we bin the data into quantiles along the distribution of $x_1$. For each observation $i$ we now replace their value of $x_1$ with the upper bound of the bin they fall in and use our trained model to make new predictions. Next, we repeat this with the lower bounds of the bins and thereafter calculate the difference in the predictions for each household. We take the average of these differences in prediction for each bin, which results in the Local Effect of $x_1$  on the prediction. Lastly, to obtain the Accumulated Local Effect of some bin $k$ we sum the Local Effects of all bins from $b=1$ to $b=k$ (**WHY THE ACCUMULATION? )**. In more theoretical terms we write the ALE as 
\begin{align}
    \hat{f}_{S, ALE} (x_S)=\int_{z_{0, S}}^{x_S} E_{X_C|X_S=x_S}[\hat{f}^S(X_S, X_C)|X_S=z_S]dz_S - constant, \label{eq:ale}
\end{align}
where $\hat{f}^S$ is the partial derivative of $\hat{f}$ with respect to $X_S$. \textbf{HAVE TO EXPLAIN THIS EQUATION IN MORE DETAIL - or show the estimator right away, which is more intuitive and I care about intuition?} \\
Lastly, Apley and Zhu propose to center the ALE such that the mean effect over all ALEs will be zero. Therefore, we cannot interpret the centred ALE as the MPC of households when they are young but rather it shows what effect age has on households MPC in general, i.e. the functional relationship between predicted MPC and a feature/observable characteristic. This becomes more clear when we look at the ALE estimator, which is more intuitive than the theoretical definition: 
\begin{align}
    \hat{\tilde{f}}_{j, ALE}(x)=\sum_{k=1}^{k_j(x)} \frac{1}{n_j(k)}\sum_{i:x_{i,j}\in N_j(k)}[f(z_{k,j}, x_{i,\setminus j})-f(z_{k-1,j}, x_{i,\setminus j})] \label{eq:ale_estim}
\end{align}
This is the uncentered accumulated local effect as described before where $z_{k, j}$  represents the upper boundary of bin $k$ when looking at the ALE of variable $x_j$. $x_{i,\setminus j}$ contains all other features of observation $i$. Therefore, part most to the right represents obtaining the prediction for observation $i$ for the upper and lower bound of bin $k$ and taking the difference. Then we take the average over all observations within the neighbourhood $N_j(k)$ which are simply all observations in bin $k$. Lastly, we accumulate the single local effects by summing the local effect from the first bin $k=1$ to the bin in which x falls into. We then centre the ALE by subtracting the average of the ALE 
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n\hat{\tilde{f}}_{j, ALE}(x_{i,j})
\end{align*}
which is simply the average of the ALE at every actually existing value of $x_j$.\\ 
Moreover, note that we yet cannot say something meaningful about the significance of these results. A specific approach to retrieve Standard Errors using these methods has not been developed yet. A deeper look into this topic is, however, out of the scope of this work. To briefly dive into the topic of statistic significance, we use a bootstrapping based approach. We simulate the ALEs for $n_{bootstrap}$ samples. These create an empirical distribution (need at least e.g. 500-1000) on which basis we calculate pseudo-standard errors. Figure X.X reports the Confidence Intervals using the reverse percentile approach (see Appendix X.X) and the mean point estimates in each bin. We see that these bands are very wide in certain parts - especially in areas where there is a small number of observations. We strongly encourage a deeper investigation of the statistical properties of ALEs and a potential way of quantifying their uncertainty in a more rigorous way. While the ALEs show us the relationship between a specific variable and our predictions, understanding whether this relationship is statistically significantly different from playing no role would be a major improvement. 