\section{Understanding the roots of heterogeneity} \label{sec:roots_of_heterogeneity}
In the previous section we discussed the conditional average treatment effect of each individual given their specific set of characteristics. Similarly to prior contributions, we also looked at correlations between the significance of the estimated MPC and households characteristics to get a glimpse into which factors play a role in the MPC. However, this approach does not reliably tell us which variables really drive the response. The correlation might very well be spurious or driven by other factors that are correlated with the characteristic we are looking at. Therefore, it is more fruitful to look at measures that can help us identify what role a variable plays in our predicted MPCs. In case of specifications using the linear DML estimator, we know that this relationship is linear by construction since the CATE is defined as a linear combination of the single effects of interactions between treatment and the respective variable. However, the causal forest based approach will help us reveal whether there are any non-linear patterns underlying in the effect of characteristics on the MPC without assuming any functional form of these patterns. \\
For this, we turn to the Machine Learning literature, which has developed a number of tools to analyse the relationship between prediction and feature. A feature is the term used for control variables that are used to predict the outcome in the Machine Learning literature. In the following, we will use this term to describe our set of variables $X$ as those are the ones we condition on when predicting the MPC. Machine Learning estimators such as random forests are blackboxes as they only provide predictions but stay quiet on which variables are important to arrive at this prediction. The literature has proposed multiple approaches that help quantify the role of a single or multiple features, some of which we will look at in the following. 

\subsection{Marginal and Partial Dependence Plots}
Two popular approaches are the usage of marginal plots (M-Plots) and Partial Dependence Plots (Friedman, 2001). Both use the same general idea to quantify the impact of a feature $x_S$ on our predictions: we replace the value of $x_S$ of each observation with some value $v_1$. For example, we predict for each individual what their MPC looks like if they had a certain age. Then we fit our trained prediction model to this "counterfactual" dataset and take the average over all these predictions. Then we continue with $x_S=v_2$ and so on, where the values $v_j$ are a grid along the distribution of $x_S$. The difference between M-Plots and Partial Dependence Plots is the distribution of all other features $X_C$ we use to average. In case of Marginal Plots, contrary to what the name might suggest, we use the conditional distribution of $X_C$ given $x_S$ 
\begin{align}
\hat{f}_M(x_S)=\int p_{X_C|x_S} f(x_S, X_C)dX_C, 
\end{align}
where $p_{X_C|x_S}$ is the conditional distribution. On the other hand, PDPs use the marginal distribution of $X_C$
\begin{align}
    \hat{f}_{PDP}(x_S)=\int p_{X_C} f(x_S, X_C)dX_C \label{eq:pdp}
\end{align}
where $p_{X_C}$ is the marginal distribution. Partial Dependence Plots are more commonly used in the Machine Learning literature as they have a sound theoretical discussion behind them, while M-Plots lack this foundation and are a more data-driven approach (\textbf{sources}). Using the marginal distribution of $X_C$ effectively marginalizes out the effect of any other variables than $x_S$ at some point $v$ and therefore reveals what impact $x_S$ has on our prediction at this point. // 
However, both M-Plots and Partial Dependence Plots suffer severe drawbacks as soon as $x_S$ is correlated with any of the features contained in $X_C$. To illustrate this, let us consider the case in which a model has only two predictors - $x_1$ and $x_2$ - and they are positively correlated. To now calculate the M-Plot of $x_1$ at $v_1$ we use the conditional distribution $p_{x_2|x_1}$. In practice, you plug in $x_1=v_1$ for each observation and take the average of predictions within a neighborhood of $x_1$, which effectively uses only values of $x_2$  that are observed within this neighborhood. Repeating this procedure for other values $v_j$ then results in the M-Plot of $x_1$. However, because the two variables are correlated, we do not know which variable drives the observed effect - if $x_1$  is increased, the values of $x_2$  we use for our predictions also increase because of the correlation. This problem is known as 'conflation'. \\
On the other hand, Partial Dependence Plots do not suffer from this problem because they use the marginal distribution of $x_2$. We use all observations of $x_2$ instead of only looking at a neighborhood in which $x_1=v_1$ and, therefore, do average out the effect of $x_2$ on our predictions. Since we use the same set of $x_2$ values at each point $v_j$, we know that changes in our predictions must stem from $x_1.$ Still, the PDPs are not a good tool when features are correlated and this is connected to the machine learning models - nonparametric estimators - we use them for. The reason is the 'counterfactual' dataset we create when using the whole distribution of $x_2$ values, which under the existence of correlation between the two features, creates observations that are extremely unlikely to exist in the observed data. This forces our predictor to extrapolate from what it has learned - something these estimators perform rather bad at (Apley and Zhu, XXX). Therefore, while the PDPs do not suffer from theoretical drawbacks like the M-Plots, in practice they are unable to uncover the effects of $x_1$ on our predictions in a stable manner because the predictions are bad. If the true model is indeed linear and we use a linear prediction method with the correct interaction terms etc., then this extrapolation issue is unlikely to occur. Moreover, by construction, a linear predictor will results in linear Partial Dependence Plots. Remember that in our linear DML approach, we assume that the CATE we estimate is linear in features $X$ and the second stage - the fitted model we actually investigate here - is simply a linear regression, which results in a linear PDP by construction as our predicted MPC is simply the sum of all coefficients for individual $i$ given their characteristics. \\
Indeed, results of the partial dependence plots are rather spurious. They are reported in more detail in Appendix X.X, where we look at the PDPs for non-durable consumption. The effects have a high variation and point estimates out of line of the existing literature. \textbf{this is not a good reasoning of why I don't show them because they are too close to the ALEs - I guess}

\subsection{Accumulated Local Effects}

To circumvent issues arising in M-Plots and PDPs from correlated features, Apley and Zhu (XXX) propose Accumulated Local Effects (ALE). As they show, ALEs can avoid both pitfalls when features are correlated. As with M-Plots we use the conditional distribution to bypass the extrapolation issues that PDPs suffer from. However, as we have already discussed, this leads to 'conflation' of the effect. To ensure that the Accumulated Local Effect only captures the effect of $x_1$ on our prediction, we look at the average marginal change of our prediction at $x_1=v_1$. Thus, we only look at the average local effect of $x_1$ on our prediction. This deals with the correlation issue in PDPs because as long as correlation is moderate (o.w. problems like multicollinearity in our estimation approach itself will arise), $x_2$ does not change as we look at a infinitesimal small change in $x_1$ and therefore the ALE recovers the local effect of $x_1$ on our prediction. Summarizing, the ALE is defined as 
\begin{align}
\hat{f}_{S, ALE} (x_S)=\int_{z_{0, S}}^{x_S} E_{X_C|X_S=x_S}[\hat{f}^S(X_S, X_C)|X_S=z_S]dz_S - constant, \label{eq:ale}
\end{align}
The above described process only recovers the average local effect, but looking at \ref{eq:ale} the 'Accumulated' in ALE becomes clear as well. Since we average the marginal changes in our prediction at $x_1$ we have to relate this measure back to our predictor itself by integrating up to the point at which we are looking at. Thus, we accumulate all local effects up to $x_1=v_j$ if we are interested in the ALE at $v_j$. \\
To estimate the ALE we use the following estimator, which illustrates the procedure in more intuitive terms: 
\begin{align}
\hat{\tilde{f}}_{j, ALE}(x)=\sum_{k=1}^{k_j(x)} \frac{1}{n_j(k)}\sum_{i:x_{i,j}\in N_j(k)}[f(z_{k,j}, x_{i,\setminus j})-f(z_{k-1,j}, x_{i,\setminus j})] \label{eq:ale_estim}
\end{align}
To estimate the ALE we bin the data into $n_b$ bins based on quantiles of the distribution of $x_S$ in our sample. As machine learning methods - such as the random forest based approach we follow in our causal forest specification - have no concept of a gradient, we mimic the partial derivative by using the difference in predictions when plugging in $x_S=x_k$ and $x_S=x_{k-1}$  for each observation, where $x_k$ is the upper bound of bin $k$. This difference is then averaged over all predictions. To account for the conditional distribution, we only use observations that fall within a neigborhood $N_k$, which is namely the bin we are estimating the ALE for. Since this only shows how a change in $x_S$ affects our predictions but we are interested not in the marginal but the total effect of $x_S$ , we sum the local effects of each bin up to bin $k$. \\

As a last step, Apley and Zhu propose to center the effect around the average of all ALEs such that the mean effect is zero. Thus, the ALE has to be interpreted relative to the average prediction and it shows whether for a given $x_S=v$ the effect of $x_S$ is above or below the average prediction. I.e. whether $x_S$ affects our predictions at $x_S=v$ more than it does on average. 

Moreover, note that we yet cannot say something meaningful about the statistical significance of these results. Most fields are only interested in the predictive power of machine learning methods and to understand how these predictions are achieved, but there is no notion of statistical significance in these settings. Therefore, a specific approach to quantify uncertainty of these measures has not yet been developed. A deeper look into this topic is, however, out of the scope of this work. To briefly dive into the topic of statistic significance, we use a bootstrapping based approach. We simulate the ALEs for $n_{bootstrap}$ samples. These create an empirical distribution (need at least e.g. 500-1000) on which basis we calculate pseudo-standard errors. Figure X.X reports the Confidence Intervals using the reverse percentile approach (see Appendix X.X) and the mean point estimates in each bin. We see that these bands are very wide in certain parts - especially in areas where there is a small number of observations. We strongly encourage a deeper investigation of the statistical properties of ALEs and a potential way of quantifying their uncertainty in a more rigorous way. While the ALEs show us the relationship between a specific variable and our predictions, understanding whether this relationship is statistically significantly different from playing no role would be a major improvement.

\subsection{Results}
Figure X.X plots the Accumulated Local Effects.