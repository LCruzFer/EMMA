\section{Understanding the roots of heterogeneity} \label{sec:roots_of_heterogeneity}
In the previous section we discussed the conditional average treatment effect of each individual given their specific set of characteristics. Similarly to prior contributions, we also looked at correlations between the significance of the estimated MPC and households characteristics to get a glimpse into which factors play a role in the MPC. However, this approach does not reliably tell us which variables really drive the response. The correlation might very well be spurious or driven by other factors that are correlated with the characteristic we are looking at. Therefore, it is more fruitful to look at measures that can help us identify what role a variable plays in our predicted MPCs. In case of specifications using the linear DML estimator, we know that this relationship is linear by construction since the CATE is defined as a linear combination of the single effects of interactions between treatment and the respective variable (see equation (\ref{eq:CATE})). However, the causal forest based approach will help us reveal whether there are any non-linear patterns underlying in the effect of characteristics on the MPC without assuming any functional form of these patterns. \\
For this, we turn to the Machine Learning literature, which has developed a number of tools to analyse the relationship between prediction and feature. Feature is a different term for control variables. In our setting these are the variables we condition on to find the CATE, i.e. variables contained in $X$. Since variables in $W$ are assumed to not impact the CATE they are not contained in the second stage and therefore play no role in predicting individuals' MPC. Machine Learning estimators such as random forests are blackboxes as they only provide predictions but stay quiet on which variables are important to arrive at this prediction. The literature has proposed multiple approaches that help quantify the role of a single feature, some of which we look at in the following. 

\subsection{Marginal and Partial Dependence Plots}
Two popular approaches are marginal plots (M-Plots) and Partial Dependence Plots (PDPs; Friedman, 2001). Both use the same general idea to quantify the impact of some feature $x_S$ on our predictions: we replace the value of $x_S$ of each observation with some value $v_1$. Then we fit our trained prediction model to this "counterfactual" dataset and take the average over all these predictions. For example, we predict for each individual what their MPC looks like if they had a certain age and average the predicted MPC. Then we continue with $x_S=v_2$ and so on, where the values $v_j$ are chosen from a grid along the distribution of $x_S$. The difference between M-Plots and PDPs is the distribution of all other features $X_C=X\setminus x_S$ we average over. In case of Marginal Plots, contrary to what the name might suggest, we use the conditional distribution of $X_C$ given $x_S$, $p_{X_C|x_S}$, to obtain the impact of $x_S$ on our prediction
\begin{align}
\hat{f}_M(x_S=v_j)=\int p_{X_C|x_S=v_j} m(x_S=v_j, X_C)dX_C, 
\end{align}
where $m$ is our predictor and $\hat{m}_M(v_j)$ is the effect at $x_S=v_j$. On the other hand, PDPs use the marginal distribution of $X_C$, $p_{X_C}$, 
\begin{align}
    \hat{f}_{PDP}(x_S)=\int p_{X_C} f(x_S, X_C)dX_C \label{eq:pdp}
\end{align}
where $\hat{m}_{PDP}(v_j)$ is the Partial Dependence of our predictor on $x_S$ at $v_j$. Using the marginal distribution of $X_C$ effectively "marginalizes out" the effect of any other variables than $x_S$ at some point $v$ and therefore reveals what impact $x_S$ has on our prediction at this point. Partial Dependence Plots are more common in the Machine Learning literature as M-Plots suffer from a severe weakness when features in $X_C$ are correlated with $x_S$. However, PDPs also fail to reliably uncover the effect of $x_S$ in such a setting. \\
To illustrate the issues arising in M-Plots and PDPs when features are correlated, let us consider a simply example. Lets say we have some predictive model $m$ that only depends on two predictors $x_1$ and $x_2$, which are positively correlated. To now calculate the M-Plot of $x_1$ at $v_1$ we use the conditional distribution $p_{x_2|x_1}$. In practice, we plug in $x_1=v_1$ for each observation that is within a specified neighborhood of $x_1=v_1$ (e.g. observations in the same quantile). Then we predict and average to obtain the M-Plot value at $x_1=v_1$. Repeating this procedure for other values $v_j$ then results in the M-Plot of $x_1$. However, because the two variables are correlated, we do not know which variable drives the observed effect - if $x_1$  is increased, the values of $x_2$  we use for our predictions also increase because we only use $x_2$ of observations that are close to having $x_1=v_j$. This problem is known as 'conflation'. \\
On the other hand, Partial Dependence Plots do not suffer from this problem because they use the marginal distribution of $x_2$. We use all observations of $x_2$ instead of only looking at a neighborhood in which $x_1=v_1$ and, therefore, do average out the effect of $x_2$ on our predictions. Since we use the same set of $x_2$ values at each point $v_j$, we know that changes in our predictions must stem from $x_1.$ Still, the PDPs are not a good tool when features are correlated and this is connected to the machine learning estimators we apply them to. These are nonparametric estimators that are usually very weak in predicting outcomes based on observations they have never seen before. This extrapolation however becomes necessary when we create the "counterfactual" dataset by setting $x_1=v_j$. By doing so, we effectviely create observations that are extremely unlikely or even impossible to observed in the real world because of the correlation the features. For example, in our data age and salary are strongly correlated, which is quite intuitive because once retired, households do not receive a salary anymore. When creating PDPs we ignore this fact and create households that have a high salary and are very old. The weakness in extrapolation leads the model to create weak predictions, which then severly bias the Partial Dependence Plots. (Apley and Zhu, 2020)  \\
Therefore, while PDPs do not suffer from theoretical drawbacks like M-Plots, in practice they are unable to uncover the effects of $x_1$ on our predictions in a stable manner because of the underlying predictive estimator. If the true model is indeed linear and we use a linear prediction method with the correct specifications of any interaction terms etc., then this extrapolation issue is unlikely to occur. Moreover, by construction, a linear predictor will results in linear Partial Dependence Plots. \textbf{Remember that in our linear DML approach, we assume that the CATE we estimate is linear in features $X$ and the second stage - the fitted model we actually investigate here - is simply a linear regression, which results in a linear PDP by construction as our predicted MPC is simply the sum of all coefficients for individual $i$ given their characteristics. $\rightarrow$ I am not so sure about this part yet} \\
Indeed, results of the partial dependence plots are rather spurious. They are reported in more detail in Appendix X.X, where we look at the PDPs for non-durable consumption. The effects have a high variation and point estimates out of line of the existing literature. \textbf{this is not a good reasoning of why I don't show them because they are too close to the ALEs - I guess}

\subsection{Accumulated Local Effects}
To circumvent issues arising in M-Plots and PDPs from correlated features, Apley and Zhu (XXX) propose Accumulated Local Effects (ALE). The extrapolation issue PDPs suffer from is bypassed by using the conditional distribution $p_{X_C|x_S}$ as we do in M-Plots. As with M-Plots we use the conditional distribution to bypass the extrapolation issues that PDPs suffer from. The 'conflation' effect that results from this is tackled by not using average predictions at $x_S=v_j$ but rather the average marginal change in predictions at this point. We apply this by using the partial derivative of our predictor $m$ with respect to $x_S$ at the point $v_j$. Although many machine learning methods such as tree based learners have no concept of a gradient, Apley and Zhu are able to derive proofs for non-differentiable functions $m$ (see Section X.X) and further does this not play a role when it comes to the ALE estimation. The ALE is then defined as
\begin{align}
\hat{f}_{S, ALE} (x_S)=\int_{z_{0, S}}^{x_S} E_{X_C|X_S=x_S}[\hat{f}^S(X_S, X_C)|X_S=z_S]dz_S - constant, \label{eq:ale}.
\end{align}
Looking at this equation step-by-step reveals how the ALE recovers the effect of $x_S$ on our predictions even when features are correlated. As already mentioned, the ALE avoids 'conflation' by using the partial derivative of $m$, where we have $m^S=\frac{\partial m}{\partial x_S}\rvert_{x_S=v_j}$ as the partial derivative of $m$ evaluated at the point we want to find the ALE for. Since we only look at an infinitesimally small change, this change in $x_S$ will not affect the features that are correlated with it in $X_C$ unless the correlation is extremely high. In our analysis we would want to avoid this case anyways to avoid problems in the estimation itself (e.g. multicollinearity). Once the changes in prediction are obtained for each observation, we average them over the conditional distribution, i.e. only using observations that are within a neighborhood of $x_S=v_j$ and actually exist. Now we have the average local effect, but we are interested in how $x_S$ affects our predictions and not how it affects changes in predictions. Thus, we simply integrate over all local effects up to $x_S=v_j$, where $z_{0, S}$ is the lower bound of the distribution of $x_S$. \\
To estimate the ALE we use the following estimator, which illustrates the procedure in more intuitive terms: 
\begin{align}
\hat{\tilde{f}}_{j, ALE}(x)=\sum_{k=1}^{k_j(x)} \frac{1}{n_j(k)}\sum_{i:x_{i,j}\in N_j(k)}[f(z_{k,j}, x_{i,\setminus j})-f(z_{k-1,j}, x_{i,\setminus j})] \label{eq:ale_estim}
\end{align}
The intuition behind the estimator is straightforward. First, we bin our data into $n_b$ bins based on quantiles of the distribution of $x_S$. To mimic the marginal change represented by the partial derivative $m^S$ in \ref{eq:ale} we make two predictions for each individual. For an observation $i$ that falls in bin $k$, we predict its outcome with $x_S=z_k$ and $x_S=z_{k-1}$, where $z_k$ represents the upper bound value of quantile $k$. We then averages over all individuals that fall within this bin $k$ and finally accumulate all predicted differences from the lowest bin up to bin $k$. Only looking at individuals within a neighborhood $N(k)$ - effectively observations in the same quantile - accounts for the conditional distribution used in \ref{eq:ale}. \\
As a last step, Apley and Zhu propose to center the effect around the average of all ALEs such that the mean effect is zero. Thus, the ALE has to be interpreted relative to the average prediction and it shows whether for a given $x_S=v$ the effect of $x_S$ is above or below the average prediction. I.e. whether $x_S$ affects our predictions at $x_S=v$ more than it does on average. In practice, the $constant$ in (\ref{eq:ale}) is replaced by $$average term here$$. \\
Note that we yet cannot say something meaningful about the statistical significance of these results. Most fields are only interested in the predictive power of machine learning methods and to understand how these predictions are achieved, but there is no notion of statistical significance in these settings. Therefore, a specific approach to quantify uncertainty of these measures has not yet been developed. A deeper look into this topic is, however, out of the scope of this work. To briefly dive into the topic of statistic significance, we use a bootstrapping based approach. We simulate the ALEs for $n_{bootstrap}$ samples. These create an empirical distribution (need at least e.g. 500-1000) on which basis we calculate pseudo-standard errors. Figure X.X reports the Confidence Intervals using the reverse percentile approach (see Appendix X.X) and the mean point estimates in each bin. We see that these bands are very wide in certain parts - especially in areas where there is a small number of observations. We strongly encourage a deeper investigation of the statistical properties of ALEs and a potential way of quantifying their uncertainty in a more rigorous way. While the ALEs show us the relationship between a specific variable and our predictions, understanding whether this relationship is statistically significantly different from playing no role would be a major improvement. \\
One weakness of ALE is further that they are a global measure, i.e. they do not help to uncover heterogeneity in the reaction of the prediction to a single variable. One method that can unbeil such heterogeneities are Individual Conditional Expectations, but they suffer from the same conceptual problems as Partial Dependence Plots. Next to investigating the role of uncertainty in the measures presented in this section we therefore also urge the development of theoretical foundations of such a measure. For now, we account for heterogeneity by plotting the unaveraged ALEs of each individual. To avoid overplotting, we only plot the households at the quintiles of the ALE distribution. (\textbf{delete last part of this if not doing it!!})

\subsection{Results}
We now turn to the analysis of the Accumulated Local Effects of a selection of features used in our estimated specifications. As pointed out in Section 6.1, by construction of the estimator and how the ALE is calculated, it will always depict a linear relationship when looking at the linear DML setting. However, we can still infer in what direction the relationship is going - e.g. whether predictions are above or below average for young people. More importantly, it is useful as a benchmark to compare the ALE of our estimates using the causal forest as the second stage estimator. \\
Two main channels of MPC heterogeneity discussed in the literature are the role of age and liquidity. We start by investigating the role of age and plot the ALE for households' age with respect when using changes in non-durable consumption as the outcome in Figure X.X. Two things become evident right away: in the linear model, the MPC is increasing in age, while this relationship turns around once we control for liquidity. The correlation between the two factors seems to play a strong role in the predicted MPC and not controlling for liquidity leads it to act through age. However, the deviation from the average prediction for old and young households decreases and is very close to zero. The strong effects of age therefore vanish once we include liquidity. Comparing these results with the causal forest based predictions, offers a good view on the underlying non-linearities ignored by the other models. Even when not controlling for liquidity in the first two specifications, we see that the strong relationship between age and prediction is less existent in the non-linear model. Rather we see a slight increase in predictions for middle-aged households. However, once we control for liquidity, the causal forest based predictions support the evidence from the linear model that MPCs of older households are higher compared to the average responses. Summarizing the ALEs of age it is reasonable to say that the linear model fails to account for the correct relationship between age and MPC, while our findings support that households at the upper end of the age distribution experience higher MPCs out of the rebate shock than on average. As we have laid out in Section 6.1 we cannot make any substantial claims on whether these reactions are significantly different from zero but rather only look at the role age plays for our MPC predictions. In our case this means that a higher age implies that a household's MPC will be larger than the average MPC of other households. \\
The main channel identified in the literature so far is liquidity. Our discussion of the underlying theory of binding borrowing constraints and lacking access to liquid assets provides the intuition for the following analysis. We consider the change in non-durable consumption here, but note that this pattern is evident across all main consumption categories. ALE plots for these can be found in Appendix A.A. Figure X.X provides the ALE plots for specification 3, which includes liquidity, income and salary, for both estimation procedures. \\
Using the linear estimator, we find that the predictions rise in liquidity, however, the deviations calculated are very small - remember though we cannot test whether they are significant. The causal forest estimations are more informative. Here we see a strong spike in low levels of liquidity that falls of as rapidly as it rises once liquidity is sufficiently large (around 5000). This underlines the important role of liquidity documented in the literature and by our results presented in Section 5.2 where we have seen that controlling for and conditioning on liquidity has a large impact on the MPCs and their significance. The ALE now provides further hints on what the role of liquidity might look like. We see that for very low levels of liquidity, the reaction of households is even below average, while once it is slightly above zero, we see that there is a steep jump in the ALE, which more or less declines immediately again at increasing levels of liquidity. This potentially signals that households with no liquid assets at all will actually not react more strongly than the average household. Since we find that the average household does not significantly react we infer from this that households with no liquidity are not reacting to the income shock. In our data income and salary are positively correlated, i.e. households with low liquidity also have low income. Thus, it is reasonable to assume that for these households the rebate checks make up a larger share of their total income within this quarter. Given the times of economic hardship for many of these households during the time of the survey, it is possible that these households did not spend a significant amount all within one quarter but stretched out their expenditures over several months, using the rebate checks as fall back savings. However, this is only one interpretation of our results and our data is too limited to infer robust causal relationships and the existence of such channels. \\
The reaction for low liquidity households is in line with our expectations from the liquidity channel we discussed. As soon as households have enough liquid assets it seems that they are capable of borrowing to smooth out the income shock before it arrives and thus show no significant reaction in consumption. \\
Last, we would like to discuss the effect of income and salary on the predicted MPC. As we can see from the ALEs in Figure X.X (they display the causal forest estimates for spec 3 of chNDexp for salary and income), we observe that salary and income have a strongly negative impact on the predicted MPCs. Following Kaplan and Violante, these might explain the negative predictions we observe. This relates back to their argument that when using the tax stimulus we estimate a 'rebate coefficient' and not necessarily the MPC - a concept that is bounded by zero on the lower bound (you can only consume 0 out of every dollar earned). However, the rebate coefficient can very well be negative as they show in estimations using their calibrated two-asset model. In this model, they explain the heterogeneous response to the 2001 tax stimulus by the government by distinciting between hosueholds that are welathy but only hold illiquid assets (wealthy hand-to-mouth) and households that have no liquidity and hold no illiquid assets. 