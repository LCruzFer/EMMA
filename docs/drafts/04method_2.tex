\newpage
\section{Methodology} \label{sec:methodology}
To identify the causal effect of the income shock on households' change in consumption, we employ the Double Machine Learning (DML) approach developed by \cite{DML2017}. More precisely, to account for the dynamic structure of our data we use the variant from \cite{PanelDML} who present a DML estimator that allows for less constraining conditions. \\ 
In a setting like ours where one is interested to estimate heterogenous treatment effects, the DML estimator has a major advantage over classical econometric approaches which have been adopted in the literature so far. Namely, it does not restrict the effect of confounders on the outcome to a specific functional form but uses Machine Learning methods to freely estimate this relationship. Simultaneously, the orthogonalization step discussed below takes care of identifying the true effect stemming only from the treatment. Lastly, from a theoretical perspective this estimator yields very efficient properties when it comes to its asymptotic analysis, especially a rate of convergence that is faster than other nonparametric estimators, in part even achieving root-n consistency. However, we will not further elaborate on these lattertechnical details but rather focus on how the estimator works in general. For a more technical discussion the reader is referred to \cite{DML2017} and \cite{PanelDML}. Instead, this section introduces the general idea behind the DML estimator and how the Panel DML setup differs. 

\subsection{Idea behind DML} \label{sec:dml-idea}
\textbf{restructure this part better implement how CATE is formed and what it looks like in this setting}
We start with considering a Partially Linear Model of treatment and outcome. Note that the DML estimator is not constrained to this form but it rather helps us to understand the idea and mechanics behind it in a clear way. Section \ref{sec:NP-DML} will briefly present what a fully non-parametric approach looks like. \\ 
The Partially Linear Model (PLM) is given by
\begin{align}
    Y_{it}&=\theta(X_{it})D_{it}+g(X_{it}, W_{it})+\epsilon_{it} \label{eq:plm1}\\
    D_{it}&=h(X_{it}, W_{it})+u_{it}, \label{eq:plm2}
\end{align}
where $Y_{it}$ is the outcome, $D_{it}$ is the treatment and $X_{it}$ and $W_{it}$ are observable variables. We distinct between simple confounders $W_{it}$ which affect the outcome and also potentially the treatment and $X_{it}$ which additionally are considered to impact the average treatment effect of $D_{it}$ on $Y_{it}$. The choice of these variables is left to the researcher. \\ 
The DML now follows a two step procedure to clearly identify $\theta(X)$. First, we define 
\begin{align}
    E[Y_{it}|X_{it}, W_{it}] &\equiv f(X_{it}, W_{it}) \label{eq:EY_def}\\ 
    E[D_{it}|X_{it}, W_{it}] &\equiv h(X_{it}, W_{it}) \label{eq:ED_def}
\end{align}
where (\ref{eq:ED_def}) follows from (\ref{eq:plm2}). Then we can rewrite (\ref{eq:plm1}) as
\begin{align*}
    Y_{it}-f(X_{it}, W_{it})=\theta(X_{it})(D_{it} - h(X_{it}, W_{it})+\epsilon_{it}.
\end{align*}
This orthogonalization guarantees that the regression only captures variation that is not induced by any of the confounders in treatment and outcome, respectively. \\
The first stage of the estimation process consists of choosing an appropriate Machine Learning method to find estimates of the conditional expectation functions $f(\cdot, \cdot)$ and $h(\cdot, \cdot)$. A welcome property of the DML estimation is its agnostic to the first stage estimator. Thus, it allows choosing the appropriate prediction method for the given setting. \\
Once we obtain the first stage predictions $\hat{f}$ and $\hat{h}$, we use them to orthogonalize treatment and outcome to retrieve the residuals 
\begin{align*} 
    \tilde{Y}_{it}&=Y_{it}-\hat{f}(X_{it}, W_{it}) \\ 
    \tilde{D}_{it}&=D_{it}-\hat{h}(X_{it}, W_{it}).
\end{align*}
Lastly, the second stage then only consists of a simple linear regression of $\tilde{Y}_{it}$ on $\tilde{D}_{it}$ that yields $\hat{\theta}(X)$. \\ 
\textbf{Add paragraph on how CATE is estimated and on its form (see econML for example)}

\subsection{Econometric motivation behind steps} \label{sec:biases}
\textbf{this just lists thoughts, rewrite them into a proper section}
The DML estimator achieves its desirable consistency results by avoiding two common biases arising in settings that employ Machine Learning estimators: overfitting and regularization bias. \\
The latter is a concern if we simply plug in our predicted values of $\hat{f}$ into the OLS regression. As \cite{DML2017} show this leads to a substantial bias that does not vanish as the sample size increases. This motivates the orthogonalization using the first stage. It takes care of confounding effects of $X$ and $W$ but does not yield this bias. \\ 
Overfitting is the result of an estimator adjusting too much to the given data such that when it is exposed to new data it performs very badly resulting in a high variance of this estimator. Again, \cite{DML2017} show how this variance term does not vanish in the asymptotic analysis leading to an inconsistent estimator. However, this issue can be avoided using a technique called \textit{cross-fitting}. Instead of using all observations to find the estimates of $f$ and $h$ and then estimate $\theta(X)$ using the whole sample, consider the case in which we split the sample into two. The first sample is used to estimate the first stage estimators. Those are used to predict the values in the second sample, which are then subsequently used in the second stage estimation. The procedure is repeated with the roles of the samples reversed and the two resulting estimators are averaged to find the final estimate $\hat{\theta}(X)$. The cross-fitting procedure for splitting up the sample into any K folds is described in Algorithm 1 which summarizes the baseline DML estimator overall. \\
\\
\textbf{This has to look better and be more 'algorithmic'.}
\begin{algorithm}
    \caption{Double Machine Learning Estimator}
    \begin{algorithmic}[1]
        \State Split up sample into K folds. 
        \State To estimate $\widehat{h}$ and $\widehat{f}$ for for the $k^{th}$ fold use observations $j \notin k$. 
        \State To get residuals for observations in $k$, calculate $\widehat{h}(X_i)$ and $\widehat{f}(X_i, W_i)$ for $i \in k$ and use to retrieve residuals.
        \State Once residuals of each fold retrieved, estimate $\theta(X_i)$.
    \end{algorithmic}
\end{algorithm}

\subsection{Panel DML} \label{sec:Panel DML}
So far we have considered the original DML estimator that considers a cross-sectional setting, however, we will look at a dynamic panel of households. However, under the assumption of strict exogeneity, it is also reasonable to use this estimator in our setting. The reasoning in favor and against this assumption have been laid out in Section X. Since we aim to compare how well this assumption holds up contrary to when relaxing it, we have to introduce the Panel DML approach by \cite{PanelDML}, which only assumes conditional sequential exogeneity. More sepcifically, we assume 
\begin{align*}
    E[\epsilon_{it}|X_{it}, W_{it}, \Phi_{t}]&=0 \\ 
    E[u_{it}|X_{it}, W_{it}, \Phi_{t}]&=0
\end{align*}
where $\Phi_t$ is the information set in period $t$. Under some more technical assumptions, Semenova et al. show that in a setting with low-dimensional treatment, the estimator is similar to the original DML. The only difference in the estimation procedure is the cross-fitting algorithm in the first stage. The folds for the cross-fitting procedure are formed based on the time index instead of simply randomly splitting up the sample. \\ 
In practice, we will include lagged values of treatment and outcome as confounders in $W_{it}$ to model the information set $\Phi_{t}$. This procedure is also proposed by Semenova et al. 

\subsection{Nonparametric DML} \label{sec:NP-DML}
So far we have considered the case of a partially linear model of treatment and outcome. While this allows for flexible estimation of the confounders, it still assumes a linear relationship between the treatment and outcome, resulting in the heterogeneous treatment effect $\hat{\theta}(X)$ being linear in X. Later on however, we also propose a specification in which we make no assumption on the functional form of the heterogeneity (Specification X in section X). This nonparametric approach has the same first stage as the PLM based one, but estimates the second stage using the Causal Forest estimator proposed by Athey and Wager (????). The Causal Forest is a generalization of the Random Forest prediction method developed by Breimann (2001), which has found application in a wide array of predictive tasks. However, the original algorithm - as most Machine Learning methods focusing on prediction - does not allow for any causal inference. The Causal Forest solves this problem by generalizing the objective function to fit a treatment effect study framework and developing theory that allows retrieving standard errors of the estimated coefficients. (\textbf{obviously re-write but no more detail in general I guess}) \\ 
Using the CF as a second stage enables us to estimate the model 
\begin{align*}
    Y_{it}&=g(D_{it}, X_{it}, W_{it})+\epsilon_{it} \\ 
    D_{it}&=m(D_{it}, X_{it}, W_{it})+u_{it}.
\end{align*}
There are no specific assumption in what what form the treatment effect will depend on the confounders $X_{it}$. As part of our analysis we will compare the results to check whether the relationship is indeed linear or whether we discover non-linear heterogeinities that the PLM approach does not account for and have not been considered in literature yet.