\newpage
\section{Methodology} \label{sec:methodology}

\subsection{Notes on Methodology for writing}
\begin{itemize}
    \item see: \url{https://econml.azurewebsites.net/_autosummary/econml.dml.DML.html#econml.dml.DML}
    \item how is the CATE achieved in second stage?
    \begin{itemize}
        \item second stage in Linear DML is OLS regression 
        \item CATE is achieved through interaction of some mapping $\phi(X)$ of the confounders $X$ with the 'base' treatment effect $\Theta$.
        \item effectively running an OLS regression with interaction terms $\tilde{D} \otimes \phi(X)$ 
        \item hence, assume that CATE is linear in X unless using polynomial mapping $\phi(X)$ 
        \item nonparametric DML such as Causal Forest DML circumvents this assumption and detects non-linear heterogeneity in case there is any (run this as kind of robustness check/better specification)
    \end{itemize}
    \item causal forests/generalized random forests: 
    \begin{itemize}
        \item random forests only allow for prediction, no inference possible 
        \item Athey and Wager (2016) and Athey, Tibshirani and Wager (2019) develop causal forests/grf
        \item these allow to estimate any desired local moment equation - e.g. those of a treatment effect analysis 
        \item more specifically, in the DML case the moment equation estimated is 
        \begin{align*}
            E\big[\left(Y-E[Y|X, W] - <\theta(X), T-E[T|X, W]> - \beta(x)\right) (T; 1)|X=x\big]=0
        \end{align*}
        \item I should push this explanation into the Appendx though and just explain the GRF in more general terms (it detects heterogeneity that is not specified before)
    \end{itemize}
\end{itemize}
To identify the causal effect of the income shock on households' change in consumption, we use the Double Machine Learning (DML) estimator developed by \cite{DML2017}. This rather new kind of estimator allows to efficiently estimate semi- or non-parametric models of treatment effects. The DML estimator has the major advantage that it does not restrict the effect of confounders on the outcome to a specific functional form. Instead it uses Machine Learning methods to freely estimate this relationship. Through the orthogonalization step discussed below it takes care of any confounding effects and cleanly identifies the pure effect of treatment on the outcome. Meanwhile, its implementation procedure deals with common biases arising in more naive estimation procedures that employ Machine Learning methods, opening the door to making use of these spohisticated methods in causal inference studies. Even in settings in which the raw predictions of the ML alogrithms used are not of high quality, the estimator yields desirable results and properties (\textbf{rephrase this last sentence again}).\\
From a more theoretical perspective the DML estimator also yields very efficient properties when it comes to its asymptotic analysis, especially a rate of convergence that is faster than other nonparametric estimators. Under certain assumption, \cite{DML2017} are able to prove root-n consistency of the estimator. However, we will not further elaborate on these lattertechnical details but rather focus on how the estimator works in general. For a more technical discussion the reader is referred to \cite{DML2017} and \cite{PanelDML}. Instead, this section introduces the general idea behind the DML estimator as well as the different variants we will use in Section \ref{sec:estim_res}. 

\subsection{Idea behind DML} \label{sec:dml-idea}
We start with considering a Partially Linear Model of treatment and outcome. Note that the DML estimator is capable of estimating various models, but we will also use a PLM specification later on. Additionally, it helps to intuitively understand the main idea of the DML estimator and its mechanics. Section \ref{sec:NP-DML} will briefly present what a fully non-parametric approach looks like. \\ 
The Partially Linear Model (PLM) is given by
\begin{align}
    Y_{it}&=\theta(X_{it})D_{it}+g(X_{it}, W_{it})+\epsilon_{it} \label{eq:plm1}\\
    D_{it}&=h(X_{it}, W_{it})+u_{it}, \label{eq:plm2}
\end{align}
where $Y_{it}$ is the outcome, $D_{it}$ is the treatment and $X_{it}$ and $W_{it}$ are observable variables. We distinct between simple confounders $W_{it}$ which affect the outcome and also potentially the treatment and $X_{it}$ which additionally are considered to impact the average treatment effect of $D_{it}$ on $Y_{it}$. The choice of these variables is left to the researcher. We are interested in $\theta(X)$, the conditional average treatment effect (CATE), which in Rubin's \textbf{missing citation here} potential outcomes framework is defined as 
\begin{align*}
    \theta(X)=E[Y_1 - Y_0 | X=x].
\end{align*}
where $Y_d$ is the outcome when treatment is $D=d$. In our setting, treatment is not binary but constant, hence $\theta(X)$ represents the marginal CATE 
\begin{align*}
    \theta(X)=E[\frac{\delta Y(d)}{\delta d} | X=x].
\end{align*}
The marginal CATE measures how much a marginal increase in the continuous treatment changes the outcome, conditional on the individual having a set of characteristics $X=x$. \\
The DML now follows a two step procedure to identify $\theta(X)$. We define 
\begin{align}
    E[Y_{it}|X_{it}, W_{it}] &\equiv f(X_{it}, W_{it}) \label{eq:EY_def}\\ 
    E[D_{it}|X_{it}, W_{it}] &\equiv h(X_{it}, W_{it}) \label{eq:ED_def}
\end{align}
where (\ref{eq:ED_def}) follows from (\ref{eq:plm2}). We can use these two conditional expectations to show that the PLM can be boiled down to
\begin{align*}
    Y_{it}-f(X_{it}, W_{it})=\theta(X_{it})(D_{it} - h(X_{it}, W_{it}))+\epsilon_{it}.
\end{align*}
This orthogonalization of treatment and outcome guarantees that the regression coefficient $\theta(X)$ only captures variation that the treatment invokes in the outcome and is free of any confounders acting through treatment $D$. Moreover, as \cite{DML2017} show, simply plugging in estimates of $f$ and $h$ into the PLM results in a regularization bias when estimating $\theta(X)$ that does not vanish asymptotically, effectively prohibiting consistency of the estimator. It is circumvented by the orthogonalization of treatment and outcome, while there is no difference in orthogonalization and controlling for confounders. \textbf{(Frisch-Waugh-Lovell Theorem kind of, also potentially missing details on orthogonalization)} \\
The first stage of the estimation process consists of choosing an appropriate Machine Learning method and finding estimates of the conditional expectation functions $f$ and $h$. A welcome property of the DML estimation is its agnostic to the first stage estimator. Thus, it allows choosing the appropriate prediction method for the given setting. \\
Once we obtain the first stage predictions $\hat{f}$ and $\hat{h}$, we use them to orthogonalize treatment and outcome to retrieve the residuals 
\begin{align*} 
    \tilde{Y}_{it}&=Y_{it}-\hat{f}(X_{it}, W_{it}) \\ 
    \tilde{D}_{it}&=D_{it}-\hat{h}(X_{it}, W_{it}).
\end{align*}
The second stage then only consists of a linear regression of $\tilde{Y}_{it}$ on $\tilde{D}_{it}$ that yields $\hat{\theta}(X)$. More precisely, the partially linear model we consider here implicitly assumes a parametric form of the CATE 
\begin{align*} 
    \theta(X)=\phi(X) \times \Theta,
\end{align*} 
where $\Theta$ is the base treatment effect and $\phi(X)$ is a mapping of confounders $X$. In practice, the estimator boils down to a linear regression which includes interaction terms $\tilde{D} \otimes \phi(X)$. The mapping $\phi(X)$ can take any parametric form we might think of. In Section \ref{sec:NP-DML} we discuss how this assumption can be avoided, allowing to detect any heterogeneity without pre-defining its functional form. 

\subsection{Cross-Fitting} \label{sec:cross-fitting}
The DML estimator achieves its desirable consistency results by avoiding two common biases arising in settings that employ Machine Learning estimators: overfitting and regularization bias. We have already briefly discussed that the regularization bias is avoided through orthogonalization. However, the overfitting problem needs to be adressed on its own. \\
Overfitting is the result of an estimator adjusting too much to the given data such that when it is exposed to new data it performs very badly resulting in a high variance of this estimator. Again, \cite{DML2017} show how this variance term does not vanish in the asymptotic analysis leading to an inconsistent estimator. However, this issue can be avoided using a technique they coin \textit{cross-fitting}. Instead of using all observations to find the estimates of $f$ and $h$ and then estimate $\theta(X)$ using the whole sample, consider the case in which we split the sample into two. The first sample is used to estimate the first stage estimators. Those are used to predict the values in the second sample, which are then subsequently used for orthogonalization and the second stage estimation. In case we are interested in the unconditional average treatment effect (ATE), this procedure is repeated with the role of the samples reversed and the resulting estimators are averaged. However, in the CATE case we are interested in individual-level point estimates. Therefore, while the role of both samples are switched, we do not average any results but keep the individual level estimates of all observations. The cross-fitting procedure for splitting up the sample into any K folds is described in Algorithm 1 which summarizes the baseline DML estimator overall.\footnote{Note that \cite{DML2017} argue that K=4 or K=5 performs reasonably well, even for smaller samples.} 
\newpage
\textbf{This has to look better and be more 'algorithmic'.}
\begin{algorithm}
    \caption{Double Machine Learning Estimator}
    \begin{algorithmic}[1]
        \State Split up sample into K folds. 
        \State To estimate $\widehat{h}$ and $\widehat{f}$ for for the $k^{th}$ fold use observations $j \notin k$. 
        \State To get residuals for observations in $k$, calculate $\widehat{h}(X_i)$ and $\widehat{f}(X_i, W_i)$ for $i \in k$ and use to retrieve residuals.
        \State Once residuals of each fold retrieved, estimate $\theta(X_i)$.
    \end{algorithmic}
\end{algorithm}

\subsection{Panel DML} \label{sec:Panel DML}
So far we have considered the original DML estimator that relies on the assumption of strict exogeneity. While even in the panel setting at hand this assumption might be reasonable, \cite{PanelDML} propose another DML estimation method that relaxes it. More precisely, for their estimator to be consistent we only have to assume conditional sequential exogeneity, which enables us to control for panel dynamics in a more precise manner. We lay out why the assumption of strict exogeneity might be reasonable in Section \ref{sec:lit} and will compare these arguments to assuming conditional sequential exogeneity in Section \ref{sec:estim_res}. \\ 
More formally, in the Panel DML setting we assume
\begin{align*}
    E[\epsilon_{it}|X_{it}, W_{it}, \Phi_{t}]&=0 \\ 
    E[u_{it}|X_{it}, W_{it}, \Phi_{t}]&=0,
\end{align*}
where $\Phi_t$ is the information set in period $t$. \cite{PanelDML} show in a setting with low-dimensional treatment, this assumption still results in the same second-stage estimator as the original DML. The only difference in the estimation procedure is the cross-fitting algorithm in the first stage. We form its folds based on the time index instead of simply randomly splitting up the sample. Moreover, we can include lagged values of treatment and outcome to account for the information set $\Phi_t$. As discussed in Section \ref{sec:lit} including lagged treatment actually improves the proper identification of the MPC. 

\subsection{Nonparametric DML} \label{sec:NP-DML}
In the PLM setting, the functional relationship how the CATE is influenced by confounders $X$ via the mapping $\phi(X)$. However, the DML estimator also enables us to use a nonparametric approach that can detect any interaction between treatment and confounders to uncover heteroegeneity. It has the same first stage, but estimates the second stage using the Causal Forest estimator proposed by Athey, Tibshirani and Wager (????). The Causal Forest is a generalization of the Random Forest prediction method developed by Breimann (2001), which has found application in a wide array of predictive tasks. However, the original algorithm - as most Machine Learning methods focusing on prediction - does not allow for any causal inference. The Causal Forest solves this problem by generalizing the objective function to fit the potential outcomes framework and developing theory that allows retrieving standard errors of the estimated coefficients. Appendix A elaborates in more detail how the Causal Forest algorithm works and how it identifies the treatment effect. Using the CF as a second stage enables us to estimate the model 
\begin{align*}
    Y_{it}&=g(D_{it}, X_{it}, W_{it})+\epsilon_{it} \\ 
    D_{it}&=m(D_{it}, X_{it}, W_{it})+u_{it}.
\end{align*}
As part of our analysis we will compare the results to check whether the relationship is indeed linear or whether we discover non-linear heterogeinities that the PLM approach does not account for and have not been considered in literature yet. However, note that when using a nonparametric second stage the convergence rate of the estimator declines. While still achieving faster rates than most other nonparametric estimators, this implies that the Causal Forest based approach is more demanding when it comes to the number of observations. 