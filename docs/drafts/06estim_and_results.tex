\section{Estimation and Results} \label{sec:estim_res}
We implement the partially linear model as presented in Section X.X to estimate the effect of receiving a tax rebate of $R_{it}$ on change of consumption $\Delta C_{it}$
\begin{align}
    \Delta C_{it+1}&=\theta(X_{it})R_{it+1}+g(X_{it}, W_{it})+\epsilon_{it} \label{eq:plm_C1}\\
    R_{it}&=h(X_{it}, W_{it})+u_{it} \label{eq:plm_C2}
\end{align}
Which variables are included as confounders $X_{it}$ and $W_{it}$ and which make up $X_{it}$ depends on our specification and is named in the section discussing the respective results. In each specification we include monthly dummies to account for seasonality. They also capture any unobserved effects that might appear when households learn about the rebate, i.e. in line with Parker et al./Misra and Surico our estimation uncovers the effect of actually receiving the rebate and not the global rebate effect.

\subsection{Identifying the Income Shock} \label{subsec:identification}
Since we use the same data and event to estimate the MPC our identification is based on the approach by Parker et al. (2013). The main factor is the design of the stimulus rollout, which we can exploit to identify the income shock. The tax stimulus was paid out to households over several weeks as administrative and technological restrictions made it impossible to pay out all rebates at once. Instead the date of rebate receipt depended on the last two digits of tax filers' social security number. Therefore, we observe rebate receipts at different points in time, which allows us to use all other households that received their rebate in a different quarter as the control group. \\
\textbf{the following is probably to harsh self-criticism and too much into econometric stuff; on the other hand the definition of the control group is quite crude}
This definition of the control group is obviously problematic. Question: deal with this here or have a paragraph in the end that deals with shortcomings of the identification section? No matter where, it should go something like this: 
This definition of a control group is potentially problematic as it also includes households that already received their rebate in a prior period. This might bias our results if there are long term effects of receiving the rebate that spill into the next period. Also, it leaves the consumption change from t-1 to t in the control group biased in case households receiving the rebate in t-1  actually respond positively to the rebate. Then the change in the control group is likely to be negative as in the next period households resume their smoother consumption pattern and therefore show a negative consumption change although this is due to the control group. This biases the effect of receiving the rebate upwards as even when reactions are small the divergence is larger than the reaction itself (thisreads shaky and I am not sure whether I should include such harsh criticism of my own work here).
However, Parker et al. (2013) (or was it MS?) include a one period lag of rebate in their analysis and find no significant role of it in determining consumption change. 
\\
In the following, we will slightly depart from Parker et al.'s identification strategy given their findings as well as our inclusion of more control variables. They argue that using the actual amount of tax rebate received can lead to an omitted variable bias. This concern arises because of how households' stimulus payments are determined. First, they depend on the number of children as each dependent child adds 300 USD to the stimulus amount. Second, the stimulus excluding the child bonuses equals the household's net income tax liability (NTL; in the following also referred to as the net tax liability) as long as it is within the exogenously defined boundaries we discussed in Section 3.1. Parker et al. now argue that the NTL might also drive changes in consumption, rendering the treatment endogenous. Their solution is to instrument the amount received with a dummy variable that only signals whether the stimulus was received or not in the given quarter. While their results and the authors themselves suggest that this is not much of a concern, we decisively disagree with their identification approach. With respect to the first concern, the number of children is reported in the CEX and is easily controlled for as it is collected in each interview conducted. The role of the NTL is more complicated. Parker et al. do not control for any variable related to households income or salary, These variables are without a doubt directly connected to our treatment because the NTL is determined by income. Exlcuding these variables leads to an omitted variable biases that leads to inconsistent estimates. However, other than through the channel of income, we deem it highly unlikely that the net tax liability itself is driving changes in consumption. It might be possible that in other years the NTL plays a role for households income as it can be perceived as an anticipated income - or liquidity - shock (\footnote{Households usually should know that they will have to pay this/receive this because of past experience and because the NTL is also depending on how much income tax was already paid during the previous year.}) However, in 2008 the NTL affected households via their tax rebate, i.e. it does not affect the consumption change through other channels than what is captured by the tax rebate. Therefore, we argue in favor of using the actual rebate amount since it has two advantages: for one, we have an additional source of variation and second it allows us to estimate the continuous treatment effect and interpret it as the actual MPC in dollar amount. \\
However, one drawback of the already mentioned lack of detailed documentation of household characteristics in the CEX is the fact that once we include financial variables - most importantly liquidity and salary - our sample size shrinks because they are not consistently documented for each interviewed households. Although the DML framework achieves fast convergence rates even in cases in which the first stage predictions do not converge as rapidly, we have to keep this drawback in mind. However, contrary to Misra and Surico, we actually recover the conditional distribution of the MPC - obviously under the assumption that we control for any relevant confounders **(NOT A SENTENCE YET AND THINK ABOUT THIS IN MORE DETAIL)** - ****and contrary to Parker et al. we do not have to rely on defining our own cutoffs to detect heterogeneities. 

\subsection{Main Results}
In total, we estimate four different specifications for each outcome. By specification we refer to which variables we include as confounders $X$ and $W$ whereas we additionally distinct between the estimator used for the second stage. Thus, we estimate each specification-outcome pair twice, once using the linear and once using the causal forest (cf) second stage. Our estimation procedure enables us to recover individual level MPCs and standard errors. Therefore, we are able to test for each household whether their MPC out of the tax rebate is statistically significantly different from zero. We analyse our results in several steps but we begin with looking at the empirical distribution of the estimated MPCs. Figure X.X shows the distribution of MPCs for the four expenditure categories Parker et al. look at: Food (FD), Strictly Non-Durables (SND) as defined by Lusardi (1996), Non-Durables (ND) and Total (TOT) expenditure. The sub-components of each category are listed in Appendix A.A. Let us point out that the difference between SND and ND consumption categories are that the non-durable category also includes components that are referred to as 'semi-durables' such as health expenditures and apparel. \\
A single plot of the empirical distribution in Figure X.X is retrieved as follows: we bin the range between the lowest and highest point estimate of the MPC into 20 equally large bins and calculate the share of point estimates that fall into this range. The x-axis signals the borders of the different bins and the y-axis shows the respective frequency where the blue bar signals what the total frequency of this bin is. Additionally, we calculate for each bin how many of the individual MPCs that fall into it are statistically significant at the 10\% level and calculate the share of statistically significant estimates **within** this bin. This is depicted by the read overlay over the frequency bars. I.e. a completely red bar implies that all observations within this bin are statistically significant whereas a bat that is only red up to half of its height signals that only half of the point estimates within this bar are statistically significant. The vertical dashed line marks the average CATE - i.e. the average treatment effect across all households as a benchmark. The description notes whether this ATE is significant or not. \\ 
First, we have a look at common trends across all specifications and expenditure categories before we start taking a closer look at each category and estimation procedure. \\
Across all specifications, outcomes and estimators we find a strong pattern of heterogeneity which underlines the importance of accounting for MPC heterogeneity. The heterogeneity looks similar to what is suggested by Kaplan and Violante (2014) and Misra and Surico, with a large number of households showing no significant response to the tax rebate and the point estimates being most densely distributed close to zero, but a certain fraction of households (CHECK WHAT RATIOS ARE!!!) in our test set are experiencing strong, significant responses. Most importantly, we find that the ATE is always very close to zero but the individual point estimates show a completely different pattern. We see that the ATE falls into bins that have the (almost) highest frequency - which makes sense by construction - but note that across all estimations the respective bin never contains more than 15\% of all point estimates. This highlights the weak representativeness of the ATE and its inability to reliably assess the success of programs such as the 2008 tax stimulus. \\
In general we see that introducing more controls to our estimation reduces the spread of the point estimates across all specification-outcome pairs, indifferent to which second-stage estimator is used. The change is the most pronounced once we add liquid assets, income and salary as confounders to the estimation. \\
Most notably, the responses we find for total expenditures are in part unreasonably large for a bulk of individuals. As we will later see this is probably due to the underlying composition of the total expenditure variable and our estimation approach. It is quite likely that some outliers within a specific spending category part of TOT are driving the learning behavior of our estimators. This is underlined by the fact the the causal forest estimator finds way larger responses than the linear based estimator because the causal forest cannot handle these outliers as it has not seen them before (check how much is spent on cars etc in test sample). Also, it is important to note that in both estimators the spread of the CATE is drastically reduced once we control for liquidity, salary and income - variables we expect to be closely related to the MPC. Turning to the significance of the response, we see that adding more controls also reduces the amount of significant MPCs found, suggesting that prior specifications picked up signals of the confounding factors not included and interpreting them as signals of the rebate. Concluding, it seems that our estimation procedure is quite sensitive to extreme outliers in the underlying consumption categories as for total expenditure we find extreme responses as laid out above. This notion becomes more clearly when we turn to the non-durable and strictly non-durable goods. Excluding large durable categories such as *new vehicles* immediately reduces our estimated MPCs and their spread. Moreover, looking at the ND category we see that in specification 3, which includes liquidity, the linear model fails to reject the null for all households. Interestingly though, the causal forest model still finds a small fraction of large significant MPCs. This difference suggests that there are non-linearities in the dependence of the MPC on the variables such as liquidity - and potentially with respect to their interactions - that are ignored by the linear model but picked up by the causal forest. Accounting for these non-linearities reduces the noise in the point estimates and reveals significant MPCs where the linear analysis fails to pick up any significant MPCs and it seems as if only liquidity is the underlying driver. Comparing this to the Strictly Non-Durable category there is little difference in the distribution and significance of the recovered MPCs.