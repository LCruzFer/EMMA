\section{Estimation and Results} \label{sec:estim_res}
\begin{align}
    \Delta C_{it+1}&=\theta(X_{it})R_{it+1}+g(X_{it}, W_{it})+\epsilon_{it} \label{eq:plm_C1}\\
    R_{it}&=h(X_{it}, W_{it})+u_{it} \label{eq:plm_C2}
\end{align}
where $\Delta C_{it+1}$ is change in consumption, $R_{it+1}$ is the amount of rebate received by the household and $g(X_{it}, W_{it})$ and $h(g(X_{it}, W_{it})$ are non-parametric functions of confounders. $X_{it}$ and $W_{it}$ are distinct by the assumption that only $X_{it}$ influences the marginal effect of the rebate, $\theta(X)$, while $W_{it}$ denotes the set of confounders that play no role in the effect.
\textbf{or make the following a completely new section?}

\subsection{Understanding the roots of heterogeneity} \label{subsec:roots_of_heterogeneity}
In the previous part\dots we discussed the conditional average treatment effect of each individual given their specific set of characteristics. However, from these we do not know what the role of a single variable is. To understand the impact a single characteristic has, many prior contributions rely on looking at the correlations between this characteristic and their estimated coefficients. However, while calculating correlations can give us a good hint on what households with high or low MPCs look like, it does not answer any questions on how these characteristics actually influence the MPC. Correlations between the characteristics we include in our estimations and the individual level point estimates are shown in Table X.X. However, it is more interesting to look at measures that can help us identify what role a variable plays in our predicted MPCs. \\
For this, we turn to the Machine Learning literature, which has developed a number of tools to analyze the relationship between prediction and feature(introduce feature as another word for variable). Machine Learning estimators such as random forests are blackboxes as they only provide predictions but stay quiet on which variables are important to arrive at this prediction. Contrary to this, more standard approaches such as OLS or 2SLS provide a clear picture of what role each variable play through the estimated point estimates. \\
Note that control variables are referred to as features in the Machine Learning literature. Since we are estimating the conditional average treatment effect the feature space is made up of $X$.

\subsubsection{Partial Dependence Plots - and why we don't look at them}
First, we implement Partial Dependence Plots (PDPs) developed by Friedman (2001). The idea of the PDP is rather simple: to understand the role of feature $x_S$ to achieve our predictions, we replace the value of $x_S$ with some value $v_1$ and use our trained model to find a new set of predictions. For example, we predict for each individual what their MPC looks like if they had a certain age. Once the predictions are obtained, we take the average over all predictions and continue with the next value $v_2$ and so on. This creates a plot that shows how the values of $x_S$ affect our prediction. More formally, the PDP is defined as
\begin{align*}
    \hat{f}_{x_S}=E[\hat{f}(x_S, X_C)]=\int\hat{f}(x_S, X_C)dP(X_C)
\end{align*}
where $\hat{f}$ is our predictor and $X_C$ the set of all features except $x_C$. Note that we average over the marginal distribution of this set of variables. Therefore, the PDP estimates the average marginal effect of feature $x_S$ on our prediction $\hat{f}$. Again considering our example using age, a linearly increasing PDP would suggest that older individuals have a higher MPC - or, equivalently, that the MPC is increasing in age. \\
However, averaging over the marginal distribution of $X_C$ implies that the distribution of $X_C$ at $x_C=v$  is the same as the marginal distribution of $X_C$ for any value $v$. This assumption immediately breaks down when our features are correlated. To illustrate this, consider two features $x_1$ and $x_2$ that are correlated. If we now want to create the Partial Dependence Plot of $x_1$ at a point $v$, we replace $x_1$ with $v$ for each individual and implicitly assume that the distribution of $x_2$ is the same at every possible value of $v$ since we use the marginal distribution (rewording needed). However, since our features are correlated, this is not the case and we assign the wrong probabilities to these households (**here rewording and explain better what role of marginal distribution is/how this goes wrong in estimator)**. Instead, the distribution of $x_2$ might look very different for $x_1=v_1$ and $x_1=v_2$. This results in a potentially very severe bias. More intuitively, let us again consider our example with age and say that age is positively correlated with liquidity - older people are more liquid. When now looking at the PDP of age, we create high liquid, young households - households that are not observed in the real data because of the correlation. Therefore, the model does not know how to handle these households and the results are biased. \\
Indeed, results of the partial dependence plots are very spurious. They are reported in more detail in Appendix X.X, where we look at the PDPs for non-durable consumption. The effects have a high variation and point estimates out of line of the existing literature. 

\subsection{Accumulated Local Effects}
To circumvent issues arising in PDPs from correlated features, Apley and Zhu (XXX) propose Accumulated Local Effects (ALE). While the goal is the same, the approach to achieve differs substantially. ALEs average over the conditional distribution by considering bins of data, thus accounting for differences in the distribution of $x_2$ along $x_1$. We use Figure X.X to illustrate how we obtain ALEs. The figure shows $x_1$ on the x-axis and our prediction on the y-axis. First, we bin the data into quantiles along the distribution of $x_1$. For each observation $i$ we now replace their value of $x_1$ with the upper bound of the bin they fall in and use our trained model to make new predictions. Next, we repeat this with the lower bounds of the bins and thereafter calculate the difference in the predictions for each household. We take the average of these differences in prediction for each bin, which results in the Local Effect of $x_1$  on the prediction. Lastly, to obtain the Accumulated Local Effect of some bin $k$ we sum the Local Effects of all bins from $b=1$ to $b=k$. In more theoretical terms we write the ALE as 
\begin{align}
    \hat{f}_{S, ALE} (x_S)=\int_{z_{0, S}}^{x_S} E_{X_C|X_S=x_S}[\hat{f}^S(X_S, X_C)|X_S=z_S]dz_S - constant, \label{eq: ALE}
\end{align}
where $\hat{f}^S$ is the partial derivative of $\hat{f}$ with respect to $X_S$. \\ 
Moreover, note that we yet cannot say something meaningful about the significance of these results. A specific approach to retrieve Standard Errors using these methods has not been developed yet. A deeper look into this topic is, however, out of the scope of this work. To briefly dive into the topic of statistic significance, we use a bootstrapping based approach. We simulate the ALEs for $n_{bootstrap}$ samples. These create an empirical distribution (need at least e.g. 500-1000) on which basis we calculate pseudo-standard errors. Figure X.X reports the Confidence Intervals using the reverse percentile approach (see Appendix X.X) and the mean point estimates in each bin. We see that these bands are very wide in certain parts - especially in areas where there is a small number of observations. We strongly encourage a deeper investigation of the statistical properties of ALEs and a potential way of quantifying their uncertainty in a more rigorous way. While the ALEs show us the relationship between a specific variable and our predictions, understanding whether this relationship is statistically significantly different from playing no role would be a major improvement. 